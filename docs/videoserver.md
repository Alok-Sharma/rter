# Video Server Design

## Stream Concept

Here's a try on a concise and practical definition of a stream. It's supposed to help us identifying how to handle and manage many streams originating from different sources over time.

A stream is

- a single, unique, sequential, unidirectional, uninterrupted and long-running flow of data items
- a stream is generated by a single entity (the streaming source)
- a stream may be forwarded, processed, cached, and archived by multiple entities (streaming servers)
- a stream is displayed live, time-shifted or on-demand by no, one, or multiple entities (streaming clients)

This definition implies in particular
- the timeline of a stream is continuous and gap-free
- an intended or unintended interruption during generation (capture) results in the __end of a stream__
- restarting a data-flow after interruption generates a __new__ stream
- a data item (image, text, sound) can be part of multiple streams
- timeline manipulations (trim, concat) create new streams
- mixing of multiple streams creates new streams (such as mixing video with audio)

## Requirements

### Required Server Features
1. receive, forward and store live video from mobile devices and fixed cameras
2. support parallel ingest sessions from multiple sources (each with individual start times)
3. support dynamic sessions (initiated on request by a source)
4. keep an infinite archive of live streams
5. HTTPS mode
6. support stream ingest modes
  - live: HTTP POST individual encoded frames
  - upload: HTTP POST chunked video files, continuations, byte-range
7. support stream consumption modes
  - HLS: index and segment file download
  - Progressive Download: HTTP GET with byte range?
  - Download: HTTP GET?
8. support seeking and replay of archived video
9. generate, store and serve thumbnail images of each video stream
10. generate, store and serve poster image(s) of each video stream
11. authenticate streaming source and client
12. connection throttling (connections per IP per time)
13. bandwidth throttling (bytes per stream per time)
14. upload continuation (byte-range)
15. auto-compress text file types (such as m3u8, mpd)
16. check video format conformance (TS headers, H264 NALU headers, SPS, PPS)
17. websocket for uploading raw h264 video frames and TS packets (may avoid HTTP POST overhead)

### Required Video Format Features
- keyframes should be aligned with later segmentation boundaries to support pseudo-random access (2 sec?)
- no audio necessary (for now, although the downstream pipeline would support it)
- H264/AVC inside MPEG2-TS over HTTP(S) as transport
- raw H264/AVC-TS (NALU stream) over HTTP(S) as transport
- avoid re-encoding at server
- container format may be repacked into at server

### Ingest Format and Codec

The ingest format should be a H264 profile/level that is supported by mobile encoders and playable by Web browsers to avoid transcoding.

- HTML5 spec defines the following [H264 video and audio formats](http://www.w3.org/TR/2011/WD-html5-author-20110809/the-source-element.html)
  - avc1.42E01E, mp4a.40.2 (H.264 Constrained baseline profile video (main and extended video compatible) level 3.0 and Low-Complexity AAC audio)
  - avc1.4D401E, mp4a.40.2 H.264 Main profile video level 3.0 and Low-Complexity AAC audio)
  - avc1.58A01E, mp4a.40.2 (H.264 Extended profile (baseline-compatible) level 3.0 and Low-Complexity AAC audio)
  - avc1.64001E, mp4a.40.2 (H.264 'High' profile video (__incompatible__ with main, baseline, or extended profiles) level 3.0 and Low-Complexity AAC)
- Android support : avc1.42001E (H.264 Baseline Profile Level 3.0)
- [iOS support][2]: Baseline, Main 3.0/3.1 (>=iOS 4.0), Baseline 4.1, Main 3.2/4.1 (>= iOS 5.0), High 4.0/4.1 (iOS >= 6.0)


### Distribution Format

On the [browser side](http://wiki.whatwg.org/wiki/Video_type_parameters#Browser_Support) it seems H264 is supported by all major browsers on all major desktop and mobile platforms (2013.03)

- __iOS__ >= H.264 Baseline 3.0, see [Apple documentation](http://developer.apple.com/library/ios/#technotes/tn2224/_index.html)
- __Android__ >= H.264 Baseline 3.0, see [supported formats](http://developer.android.com/guide/appendix/media-formats.html)
- __Chrome__ >= H.264 Baseline 3.0
- __Firefox__ >= H.264 Baseline 3.0 (>=nightly 20)
- __Safari__ >= H.264 Baseline 3.0
- __IE__ >= H.264 Baseline 3.0 (>=9.0)




## Video System Architecture
```
  +---------------+     +--------+      +------------+     +--------------+     +---------+
  |    Source     | --> | Ingest | -->  | Transcoder | --> | Distribution | --> |   Web   |
  | (Android/iOS) |  :  | Server |  :   |  (FFMpeg)  |  :  |    Server    |  :  | Browser |
  +---------------+  :  | (HTTP) |  :   +------------+  :  |    (HTTP)    |	 :  +---------+
                     :  +--------+  :                   :  +--------------+  :
                     :              :                   :                    :
            H264 in MPEG2-TS     Loopback          HLS, PNG, M3U8           HTTP
                  over HTTP       UDP/RTP             Files
```

### Server Endpoints
```
POST /v1/ingest/:videoid/avc              # live ingest point for raw H264 AVC bitstreams
POST /v1/ingest/:videoid/ts               # live ingest point for MPEG2-TS streams
PUT  /v1/ingest/:videoid/chunk            # chunked file upload ingest point
GET  /v1/videos/:videoid/mp4              # mp4 (H264, AAC) file download
GET  /v1/videos/:videoid/ogg              # ogg (Theora, Vorbis) file download
GET  /v1/videos/:videoid/m3u8             # HLS index file download
GET  /v1/videos/:videoid/mpd              # DASH index file download
GET  /v1/videos/:videoid/hls/:segment     # HLS named segment download
GET  /v1/videos/:videoid/dash/:segment    # DASH named segment download
GET  /v1/previews/:videoid/:thumbid       # preview thumbnail image download
GET  /v1/posters/:videoid/:posterid       # video poster image download
```

### Server Ingest API

The API assumes the client obtained a valid __video source token__ from a backend app, including

- UID of the to be created video stream
- optional timestamp to verify freshness
- cryptographically signed hash for authenticity and integrity verification


The ingest HTTP server accepts incoming video upload requests, checks source credentials, and starts an individual transcoder per incoming stream. The transcoder segments the stream and stores HLS compatible files at a location accessible by the distribution web server.


#### Supported formats for incoming live streams

- transport protocol: HTTP(S)
- packetisation format (signalled via appropriate MIME-TYPE)
  - raw H264/AVC Annex-B NALUs (using nalu start code)
  - H264/AVC packed in MPEG2-TS
- encoding format: 640x360 H264/AVC baseline profile 3.0, 600kbit - 1200kbit
- no audio
- see [Apple HLS Recommendations][1] for details

#### Ingest HTTP Server Tasks
- authenticate source
- prepare transcoder pipe
- configure and start ffmpeg transcoder
- pipe raw data streamed from source into transcoder
- gracefully handle source disconnection and cleanup (shutdown transcoder, remove pipe)


#### Transcoder Tasks
- write HLS/DASH segment index file
- get raw data from pipe
- optionally transcode raw data (correct GOP/I-frame distance, H264 Profile/Level)
- segment stream into files
- update index file
- push files to Distribution HTTP Server


#### Authentication
- ID of the ingest video is part of the URI
- use HTTP auth headers for token passing
- report auth error (stale token, invalid token)


#### Ingest Video Dataflow
- Initialisation
  - authenticate source
  - create directory for video id
  - send HTTP 200 on success to make source start the stream
  - start transcoder
- On incoming data
  - receive stream data (frames, etc.) from source on ingest endpoint
  - acknowledge reception HTTP 200
  - push raw data to transcoding pipe
- End of Stream
  - EOS signalling with empty HTTP PUT body from source
  - source can simply close connection (no handshake required)
  - on connection failure wait for source reconnection, and fail after a timeout


### Server Distribution API

Segment files must be entirely written before they become accessible by clients. This adds a conceptual latency of one segment duration plus time to distribute segments to downstream webservers. For on-demand video files

- segment based streaming
- use HTTP server/caching infrastructure

#### Distribution Server Tasks
- authenticate client
- deliver stored index and media files
- set proper mime types and caching headers


### Server Rate Limiting

Ingest endpoints are rate limited to avoid uploading too much data. What's limited:

- total number of parallel ingest sessions (10)
- cummulative ingest bandwidth (10000 kbit)
- connection limit per source IP (100 each 15 min)
- upload per source IP (128MB each 15min)

HTTP Response Headers
- `X-Rate-Limit-Limit`: the rate limit ceiling for that given request
- `X-Rate-Limit-Remaining`: the number of requests left for the 15 minute window
- `X-Rate-Limit-Reset`: the remaining window before the rate limit resets in UTC epoch seconds



### Examples

#### Copy Live Stream from Source to Segmented Files

This example assumes the incoming stream is already encoded at the correct H264 profile/level and that it's GOP structure (key frames) is aligned with the segmentation points. If not, segmentation will happen at key-frames resulting in segments of different duration.

```
ffmpeg -v error -re -i ${pipe_path}/${video.uid}.stream -codec copy \
-map 0 -f segment -segment_time 2 -segment_list ${index_path}/${video.uid}.m3u8 \
-segment_format mpegts -segment_list_flags +live ${segment_path}/${video.uid}-%09d.ts
```

- `-v error` log errors only
- `-re' read input at native frame rate (used for realtime live streaming, maybe not necessary when input is already realtime)
- `-i filename` input (named pipe for live streaming from HTTP server)
- `-codec copy` copy encoded audio and video tracks
- `-map 0` (in->out mapping) use the first input stream for the single output in our case
- `-f segment` output format is HLS segmented files
- `-segment_time <sec>` segment duration in seconds
- `-segment_list <m3u8-file>` index file location
- `-segment_format mpegts` use MPEG2TS as container format for segment files (required by Apple HLS)
- `-segment_list_flags +live` create a live-friendly index file
- `-segment_list_size 0` how many segments to keep in the index file (0=all)
- `-segment_list type` index file format (m3u8, csv, flat)
- `file-%09d.ts` pattern for generation of segment filenames


#### Transcode Live Stream from Source into distribution format
```
#This is TODO
./ffmpeg -v 9 -loglevel 99 -re -i sourcefile.avi -an \
-c:v libx264 -b:v 128k -vpre ipod320 \
-flags -global_header -map 0 -f segment -segment_time 4 \
-segment_list test.m3u8 -segment_format mpegts stream%05d.ts
```

- `-vstats_file file` dump video coding statistics to file.
- `-force_key_frames expr:gte(t,n_forced*2)` force key frames every 2 seconds
- `-flags -global_header` place global headers in extradata instead of every keyframe.

#### Output periodic thumbnail images
```
ffmpeg -v error -y -re -i ${pipe_path}/${video.uid}.stream -vsync 1 -r ${rate} -f image2 -s ${thumbres} ${thumbnail_path}/${video.uid}-%09d.jpg
```

- `-v error` log errors only
- `-y` overwrite output files
- `-re' read input at native frame rate (used for realtime live streaming, maybe not necessary when input is already realtime)
- `-i filename` input (named pipe for live streaming from HTTP server)
- `-vsync 1` video sync method 'cfr' to duplicate and drop frames to achieve exactly the requested constant framerate.
- `-r rate` video frame rate which may be a rational number, e.g. 0.5
- `-f image2` use image output (codec is guessed from filename extension)
- `-s res` output resolution, e.g. 160x90
- `file-%09d.jpg` filename template

[1]: http://developer.apple.com/library/ios/#documentation/networkinginternet/conceptual/streamingmediaguide/UsingHTTPLiveStreaming/UsingHTTPLiveStreaming.html
[2]: http://developer.apple.com/library/ios/#documentation/AVFoundation/Reference/AVFoundation_Constants/Reference/reference.html#//apple_ref/c/data/AVVideoCodecKey
