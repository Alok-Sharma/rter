# Video Server Design

### Stream Concept

Here's a try on a concise and practical definition of a stream. It's supposed to help us identifying how to handle and manage many streams originating from different sources over time.

A stream is

- a single, unique, sequential, unidirectional, uninterrupted and long-running flow of data items
- a stream is generated by a single entity (the streaming source)
- a stream may be forwarded, processed, cached, and archived by multiple entities (streaming servers)
- a stream is displayed live, time-shifted or on-demand by no, one, or multiple entities (streaming clients)

This definition implies in particular
- the timeline of a stream is continuous and gap-free
- an intended or unintended interruption during generation (capture) results in the __end of a stream__
- restarting a data-flow after interruption generates a __new__ stream
- a data item (image, text, sound) can be part of multiple streams
- timeline manipulations (trim, concat) create new streams
- mixing of multiple streams creates new streams (such as mixing video with audio)



### Required Server Features
1. receive, forward and store live video from mobile devices and fixed cameras
2. support parallel ingest sessions from multiple sources (each with individual start times)
3. support dynamic sessions (initiated on request by a source)
4. keep an infinite archive of live streams
5. distribute video to web browsers (streaming clients)
6. support stream consumption modes:
  - live: direct forwarding with low delay
  - time shifted (switch between live and lag, allow seeking)
  - on-demand (play from archive)
7. support seeking and replay of archived video
8. store thumbnail images for access by web browsers
9. store poster image for access by web browsers
10. authenticate streaming source and client

### Required Video Format Features
- keyframes should be aligned with later segmentation boundaries to support pseudo-random access (2 sec?)
- no audio necessary (for now, although the downstream pipeline would support it)
- MPEG2-TS or raw H264-TS over HTTP(S) as transport
- container format may be repacked into at server
- avoid re-encoding at server

### Ingest Format and Codec

The ingest format should be a H264 profile/level that is supported by mobile encoders and playable by Web browsers to avoid transcoding.

- HTML5 spec defines the following [H264 video and audio formats](http://www.w3.org/TR/2011/WD-html5-author-20110809/the-source-element.html)
  - avc1.42E01E, mp4a.40.2 (H.264 Constrained baseline profile video (main and extended video compatible) level 3.0 and Low-Complexity AAC audio)
  - avc1.4D401E, mp4a.40.2 H.264 Main profile video level 3.0 and Low-Complexity AAC audio)
  - avc1.58A01E, mp4a.40.2 (H.264 Extended profile (baseline-compatible) level 3.0 and Low-Complexity AAC audio)
  - avc1.64001E, mp4a.40.2 (H.264 'High' profile video (__incompatible__ with main, baseline, or extended profiles) level 3.0 and Low-Complexity AAC)
- Android support : avc1.42001E (H.264 Baseline Profile Level 3.0)
- iOS support: ?


### Distribution Format

On the [browser side](http://wiki.whatwg.org/wiki/Video_type_parameters#Browser_Support) it seems H264 is supported by all major browsers on all major desktop and mobile platforms (2013.03)

- __iOS__ >= H.264 Baseline 3.0, see [Apple documentation](http://developer.apple.com/library/ios/#technotes/tn2224/_index.html)
- __Android__ >= H.264 Baseline 3.0, see [supported formats](http://developer.android.com/guide/appendix/media-formats.html)
- __Chrome__ >= H.264 Baseline 3.0
- __Firefox__ >= H.264 Baseline 3.0 (>=nightly 20)
- __Safari__ >= H.264 Baseline 3.0
- __IE__ >= H.264 Baseline 3.0 (>=9.0)



### Server Ingest API

The API assumes the client obtained a valid __video source token__ from a backend app, including

- UID of the to be created video stream
- optional timestamp to verify freshness
- cryptographically signed hash for authenticity and integrity verification

#### Authentication
- use HTTP auth headers for token


#### Video Dataflow
- Initialisation
  - authenticate source
  - send HTTP 200 on success to make source start the stream
  - create HLS/DASH index files
- Thereafter
  - receive stream data (frames, etc.) from source
  - generate and segment video files
  - generate thumbnails for each segment
- End of Stream
  - source can simply close connection (no handshake required)
  - on connection failure attempt waiting to allow source reconnection, but fail after a timeout

### Server Outbound Distribution
- segment based streaming, either HLS standard or own
- use HTTP server/caching infrastructure
- file based segments must be entirely written before they become accessible by clients (adding a conceptual latency of one segment duration plus time to distribute segments to downstream webservers)


## Video System Architecture
```
  +---------------+     +--------+      +------------+     +--------------+     +---------+
  |    Source     | --> | Ingest | -->  | Transcoder | --> | Distribution | --> |   Web   |
  | (Android/iOS) |  :  | Server |  :   |  (FFMpeg)  |  :  |    Server    |  :  | Browser |
  +---------------+  :  | (HTTP) |  :   +------------+  :  |    (HTTP)    |	 :  +---------+
                     :  +--------+  :                   :  +--------------+  :
                     :              :                   :                    :
            H264 in MPEG2-TS          Loopback         HLS, PNG, M3U8            HTTP
 				        over HTTP              UDP/RTP              Files
```


An ingest HTTP server accepts incoming video upload requests, checks source credentials, and starts an individual transcoder per incoming stream. The transcoder segments the stream and stores HLS compatible files at a location accessible through the distribution web server.


#### Supported formats for incoming live streams:

See [Apple Recommendations][1]  for more details on HLS.

- transport protocol: HTTP(S)
- packetisation format (signalled via appropriate MIME-TYPE)
  - raw H264/AVC Annex-B NALUs (using nalu start code)
  - H264/AVC packed in MPEG2-TS
- encoding format: 640x360 H264/AVC baseline profile 3.0, 600kbit - 1200kbit


#### Ingest HTTP Server Tasks
- authenticate source
- prepare named pipe
- configure and start ffmpeg transcoder
- pipe raw data streamed from client into transcoder
- gracefully handle source disconnection and cleanup (shutdown transcoder, remove pipe)

#### Transcoder Tasks
- write HLS/DASH segment index file
- get raw data from pipe
- optionally transcode raw data (correct GOP/I-frame distance, H264 Profile/Level)
- segment stream into files
- update index file
- push files to Distribution HTTP Server

#### Distribution Server Tasks
- authenticate client
- deliver stored index and media files
- set proper mime types and caching headers


### Examples

#### Copy Live Stream from Source to Segmented Files

This example assumes the incoming stream is already encoded at the correct H264 profile/level and that it's GOP structure (key frames) is aligned with the segmentation points. If not, segmentation will happen at key-frames resulting in segments of different duration.

```
ffmpeg -v error -re -i ${pipe_path}/${video.uid}.stream -codec copy \
-map 0 -f segment -segment_time 2 -segment_list ${index_path}/${video.uid}.m3u8 \
-segment_format mpegts -segment_list_flags +live ${segment_path}/${video.uid}-%09d.ts
```

- `-v error` log errors only
- `-re' read input at native frame rate (used for realtime live streaming, maybe not necessary when input is already realtime)
- `-i filename` input (named pipe for live streaming from HTTP server)
- `-codec copy` copy encoded audio and video tracks
- `-map 0` (in->out mapping) use the first input stream for the single output in our case
- `-f segment` output format is HLS segmented files
- `-segment_time <sec>` segment duration in seconds
- `-segment_list <m3u8-file>` index file location
- `-segment_format mpegts` use MPEG2TS as container format for segment files (required by Apple HLS)
- `-segment_list_flags +live` create a live-friendly index file
- `-segment_list_size 0` how many segments to keep in the index file (0=all)
- `-segment_list type` index file format (m3u8, csv, flat)
- `file-%09d.ts` pattern for generation of segment filenames


#### Transcode Live Stream from Source into distribution format
```
#This is TODO
./ffmpeg -v 9 -loglevel 99 -re -i sourcefile.avi -an \
-c:v libx264 -b:v 128k -vpre ipod320 \
-flags -global_header -map 0 -f segment -segment_time 4 \
-segment_list test.m3u8 -segment_format mpegts stream%05d.ts
```

- `-vstats_file file` dump video coding statistics to file.
- `-force_key_frames expr:gte(t,n_forced*2)` force key frames every 2 seconds
- `-flags -global_header` place global headers in extradata instead of every keyframe.

#### Output periodic thumbnail images
```
ffmpeg -v error -y -re -i ${pipe_path}/${video.uid}.stream -vsync 1 -r ${rate} -f image2 -s ${thumbres} ${thumbnail_path}/${video.uid}-%09d.jpg
```

- `-v error` log errors only
- `-y` overwrite output files
- `-re' read input at native frame rate (used for realtime live streaming, maybe not necessary when input is already realtime)
- `-i filename` input (named pipe for live streaming from HTTP server)
- `-vsync 1` video sync method 'cfr' to duplicate and drop frames to achieve exactly the requested constant framerate.
- `-r rate` video frame rate which may be a rational number, e.g. 0.5
- `-f image2` use image output (codec is guessed from filename extension)
- `-s res` output resolution, e.g. 160x90
- `file-%09d.jpg` filename template

[1]: http://developer.apple.com/library/ios/#documentation/networkinginternet/conceptual/streamingmediaguide/UsingHTTPLiveStreaming/UsingHTTPLiveStreaming.html
